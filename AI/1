from llama_cpp import Llama

# Load the model (update path to your GGUF file)
llm = Llama(
    model_path="path/to/llama2-7b-nous-hermes-8-bit.gguf",
    n_ctx=4096,
    n_threads=4  # adjust based on your CPU
)

def analyze_performance_report(report_text):
    prompt = f"""
You are an expert in performance testing and root cause analysis.

Analyze the following performance test report and provide an output with:

1. **Test Summary** – Passed/Failed, overall observations.  
2. **Failure Analysis (if any)** – Identify what failed and suggest possible reasons (e.g., DB query slowness, network, CPU spike).  
3. **Performance Bottlenecks** – If test passed but response time is high, identify where the delay is noticed (e.g., login API took 3s, checkout API slowed due to DB).  
4. **Improvement Suggestions** – Recommend optimizations (e.g., add DB index, optimize API logic, enable caching).  
5. **Important Metrics** – Mention critical numbers like response time, error rate, throughput if available.

Make it concise, clear, and insightful.

Here is the performance report:
{report_text}

Your response:
"""

    response = llm(prompt, max_tokens=1024, temperature=0.3, stop=["</s>"])
    return response["choices"][0]["text"].strip()

if __name__ == "__main__":
    # Example input (you can replace or load from a file)
    report = """
    Test Name: Checkout API Performance Test
    Status: Passed with 10% error rate
    Average Response Time: 3.5s
    95th Percentile: 5.2s
    Failure Logs: Timeout error on /payment/process, DB query exceeded 4s
    CPU Usage: 85%, Memory Usage: 70%
    """

    result = analyze_performance_report(report)
    print("\n=== AI Performance Analysis Summary ===\n")
    print(result)
