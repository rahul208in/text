Here's the complete updated script with stdout suppression:

```python
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama
import torch
import os
import sys
import contextlib
import io

# Correct path format for Windows
LLM_MODEL_PATH = r"C:\Users\omish\Downloads\JUNK\models\mistral-7b-instruct-v0.1.Q4_0.gguf"
EMBEDDING_MODEL_PATH = r"C:\Users\omish\Downloads\JUNK\models\gte-large"

# Context manager to suppress stdout
@contextlib.contextmanager
def suppress_stdout():
    stdout = sys.stdout
    stdout_null = open(os.devnull, 'w')
    sys.stdout = stdout_null
    try:
        yield
    finally:
        sys.stdout = stdout
        stdout_null.close()

def verify_model_loading():
    print("\nVerifying Mistral Model Loading...")
    try:
        if not os.path.exists(LLM_MODEL_PATH):
            print(f"Error: Model file not found at {LLM_MODEL_PATH}")
            return None

        with suppress_stdout():
            model = Llama(
                model_path=LLM_MODEL_PATH,
                n_ctx=2048,
                n_threads=8
            )

        print("Model successfully loaded")
        print(f"Model path: {LLM_MODEL_PATH}")
        print(f"File size: {os.path.getsize(LLM_MODEL_PATH) / (1024*1024*1024):.2f} GB")

        return model
    except Exception as e:
        print(f"Error verifying model: {e}")
        return None

def test_llm():
    print("\nTesting Mistral 7B LLM...")
    try:
        with suppress_stdout():
            model = Llama(
                model_path=LLM_MODEL_PATH,
                n_ctx=2048,
                n_threads=8
            )

        prompts = [
            "<s>[INST] What is the capital of France? [/INST]",
            "<s>[INST] Write a simple Python function to add two numbers. [/INST]",
            "<s>[INST] Explain what is machine learning in one sentence. [/INST]"
        ]

        for prompt in prompts:
            print(f"\nPrompt: {prompt}")

            with suppress_stdout():
                response = model.create_completion(
                    prompt,
                    max_tokens=256,
                    temperature=0.7,
                    top_p=0.95,
                    repeat_penalty=1.1,
                    stop=["</s>", "[INST]"]
                )

            if 'choices' in response and len(response['choices']) > 0:
                print(f"Response: {response['choices'][0]['text'].strip()}")
            else:
                print("No response generated")

    except Exception as e:
        print(f"Error testing LLM: {e}")
        import traceback
        print(traceback.format_exc())

def test_gte_embedding():
    print("\nTesting GTE Embedding Model...")
    try:
        with suppress_stdout():
            embedding_model = SentenceTransformer(EMBEDDING_MODEL_PATH)

        sentences = [
            "I love programming",
            "I enjoy coding",
            "The weather is nice today"
        ]

        with suppress_stdout():
            embeddings = embedding_model.encode(sentences)

        print(f"Successfully generated embeddings for {len(sentences)} sentences")
        print(f"Embedding shape: {embeddings.shape}")

        similarity = torch.nn.functional.cosine_similarity(
            torch.tensor(embeddings[0]).unsqueeze(0),
            torch.tensor(embeddings[1]).unsqueeze(0)
        )

        print(f"\nSimilarity Test:")
        print(f"Sentence 1: {sentences[0]}")
        print(f"Sentence 2: {sentences[1]}")
        print(f"Similarity Score: {similarity.item():.4f}")

    except Exception as e:
        print(f"Error testing GTE Embedding Model: {e}")

def run_chat_conversation():
    print("\nTesting Chat Conversation...")
    try:
        with suppress_stdout():
            model = Llama(
                model_path=LLM_MODEL_PATH,
                n_ctx=2048,
                n_threads=8
            )

        messages = [
            {"role": "user", "content": "What is Python programming language?"},
            {"role": "user", "content": "What are its main features?"}
        ]

        for message in messages:
            with suppress_stdout():
                response = model.create_chat_completion(
                    messages=[message],
                    max_tokens=256,
                    temperature=0.7,
                    top_p=0.95
                )

            print(f"\nUser: {message['content']}")
            print(f"Assistant: {response['choices'][0]['message']['content']}")

    except Exception as e:
        print(f"Error in chat conversation: {e}")

if __name__ == "__main__":
    # Install required packages if not present
    try:
        import sentence_transformers
    except ImportError:
        print("Installing sentence-transformers...")
        import subprocess
        subprocess.check_call(["pip", "install", "sentence-transformers"])

    try:
        import llama_cpp
    except ImportError:
        print("Installing llama-cpp-python...")
        subprocess.check_call(["pip", "install", "llama-cpp-python"])

    print("Starting tests...")

    # Verify model loading
    model = verify_model_loading()

    if model:
        print("\nRunning LLM tests...")
        test_llm()
        run_chat_conversation()
    else:
        print("Skipping LLM tests due to model loading failure")

    print("\nRunning embedding tests...")
    test_gte_embedding()
```

Key changes made:
1. Added `suppress_stdout()` context manager
2. Wrapped all model loading and inference calls with `suppress_stdout()`
3. Maintained clean output for important information and responses
4. Added suppression for embedding model operations as well

This script will now run with much cleaner output, showing only the important information and responses while suppressing the model initialization messages. The functionality remains exactly the same, but the output will be much more readable.
