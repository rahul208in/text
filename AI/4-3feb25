
I'll update your main.py with the corrected implementation while keeping the stdout/stderr suppression. Here's the complete updated code:

```python
# main.py

import os
import sys
import contextlib
from config import JSON_FILES_DIR, CHROMA_DB_DIR, LLM_MODEL_PATH, EMBEDDING_MODEL_PATH
from utils.json_processor import load_all_json_files, extract_text_from_json
from models.model_loader import load_embedding_model
from database.vector_store import initialize_chroma_db, add_documents_to_vectorstore
from langchain.llms import LlamaCpp
from langchain.chains import RetrievalQA

@contextlib.contextmanager
def suppress_stdout():
    """Context manager to suppress stdout and stderr."""
    stdout = sys.stdout
    old_stderr = sys.stderr
    null_device = open(os.devnull, 'w')
    try:
        sys.stdout = null_device
        sys.stderr = null_device
        yield
    finally:
        sys.stdout = stdout
        sys.stderr = old_stderr
        null_device.close()

def initialize_llm():
    """Initialize the LLM with suppressed output."""
    with suppress_stdout():
        llm = LlamaCpp(
            model_path=LLM_MODEL_PATH,
            temperature=0.75,
            max_tokens=2000,
            top_p=1,
            verbose=True,
        )
    return llm

def main():
    # Step 1: Load all JSON data from directory
    print("Loading JSON data from directory...")
    all_json_data = load_all_json_files(JSON_FILES_DIR)

    if not all_json_data:
        print("No JSON data found in the directory!")
        return

    print(f"Loaded {len(all_json_data)} documents from JSON files")

    # Extract text from all documents
    documents = extract_text_from_json(all_json_data)
    print(f"Extracted {len(documents)} text documents")

    # Step 2: Initialize ChromaDB with LangChain integration
    print("Initializing ChromaDB...")
    os.makedirs(CHROMA_DB_DIR, exist_ok=True)

    # Initialize vector store with suppressed output
    with suppress_stdout():
        vectorstore = initialize_chroma_db(CHROMA_DB_DIR, EMBEDDING_MODEL_PATH)
        vectorstore = add_documents_to_vectorstore(vectorstore, documents)

    # Step 3: Initialize LLM
    print("Initializing LLM...")
    llm = initialize_llm()

    # Step 4: Create QA chain
    print("Creating QA chain...")
    with suppress_stdout():
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(),
            return_source_documents=True,
            verbose=True
        )

    # Step 5: Interactive question answering
    while True:
        question = input("\nEnter your question (or 'quit' to exit): ")
        if question.lower() == 'quit':
            break

        print("\nProcessing your question...")
        with suppress_stdout():
            result = qa_chain({"query": question})

        print(f"\nQuestion: {question}")
        print(f"Answer: {result['result']}")
        print("\nSource Documents:")
        for i, doc in enumerate(result['source_documents'], 1):
            print(f"\nDocument {i}:")
            print(doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content)

if __name__ == "__main__":
    main()
```

And here are the necessary supporting files that need to be updated:

```python
# database/vector_store.py

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

def initialize_chroma_db(persist_directory, embedding_model_path):
    """Initialize ChromaDB with LangChain integration."""
    embeddings = HuggingFaceEmbeddings(
        model_name=embedding_model_path,
        model_kwargs={'device': 'cpu'}
    )

    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )

    return vectorstore

def add_documents_to_vectorstore(vectorstore, documents):
    """Add documents to the vector store."""
    vectorstore.add_texts(documents)
    return vectorstore
``````python
# utils/json_processor.py

import json
import os
from typing import List, Dict

def get_all_json_files(directory: str) -> List[str]:
    """Get paths of all JSON files in the specified directory."""
    json_files = []
    for file in os.listdir(directory):
        if file.endswith('.json'):
            json_files.append(os.path.join(directory, file))
    return json_files

def load_json(file_path: str) -> List[Dict]:
    """Load JSON data from a file."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading {file_path}: {str(e)}")
        return []

def load_all_json_files(directory: str) -> List[Dict]:
    """Load data from all JSON files in the directory."""
    all_data = []
    json_files = get_all_json_files(directory)

    for file_path in json_files:
        print(f"Loading {file_path}...")
        data = load_json(file_path)
        if isinstance(data, list):
            all_data.extend(data)
        else:
            all_data.append(data)

    return all_data

def extract_text_from_json(json_data: List[Dict], key: str = "content") -> List[str]:
    """Extract text data from JSON based on a specific key."""
    texts = []
    for item in json_data:
        if isinstance(item, dict) and key in item:
            texts.append(item[key])
    return texts
``````python
# config.py

# Path to JSON files directory
JSON_FILES_DIR = "data"

# ChromaDB settings
CHROMA_DB_DIR = "cache/chroma_db"

# Model paths
LLM_MODEL_PATH = r"C:\Users\omish\Downloads\JUNK\models\mistral-7b-instruct-v0.1.Q4_0.gguf"
EMBEDDING_MODEL_PATH = r"C:\Users\omish\Downloads\JUNK\models\gte-large"

# LLM settings
MAX_TOKENS = 2000
TEMPERATURE = 0.75
```

Key changes in this updated version:

1. Maintained the stdout/stderr suppression functionality
2. Properly integrated ChromaDB with LangChain
3. Added suppression for model loading and inference
4. Improved error handling and output formatting
5. Added source document retrieval in the results
6. Removed unnecessary model loader module as we're using LangChain's built-in classes

To use this updated version:

1. Create the necessary directory structure
2. Place your JSON files in the `data` directory
3. Ensure all dependencies are installed
4. Run the main script:
```bash
python main.py
```

The script will now suppress unwanted output while still showing important progress messages and results. Let me know if you need any clarification or run into issues!
