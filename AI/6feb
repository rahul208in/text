
### **Analysis of the Issue**

From the provided screenshots and context, the following issues are identified:

1. **Same Search Results for All Queries**:
   - This indicates that the query embedding or the database search is not functioning correctly. Either the query is not being embedded properly, or the database is not storing/retrieving the data correctly.

2. **Debugging Required**:
   - We need to verify that:
     - All JSON files in the `data/` folder are being embedded and stored in ChromaDB.
     - The query is being embedded and searched in the database.

3. **Errors in Metadata and Embedding**:
   - The error `Expected metadata value to be str, int, float, or bool, got {} which is dict` suggests that invalid metadata (e.g., a dictionary) is being passed to ChromaDB. Metadata must be simple types like strings or numbers.

4. **`AttributeError: 'list' object has no attribute 'get'`**:
   - This occurs because the code is trying to call `.get()` on a list instead of a dictionary. This indicates an issue with how the JSON structure is being handled.

5. **Debugging Steps**:
   - Add debug statements to verify:
     - The data being embedded into ChromaDB.
     - The query embedding and search results.

---

### **Updated Code with Debugging**

Below is the updated code for `vector_store.py` and `main.py` with added debugging to verify the embedding and search process.

---

#### **Updated `vector_store.py`**

```python
import os
import json
from langchain_chroma import Chroma
import config


def embed_and_store_data(embeddings):
    """Embed and store data in ChromaDB."""
    vectorstore = Chroma(persist_directory=config.CHROMA_DB_DIR, embedding_function=embeddings)

    # Iterate over all JSON files in the directory
    for file_name in os.listdir(config.JSON_FILES_DIR):
        if file_name.endswith(".json"):
            file_path = os.path.join(config.JSON_FILES_DIR, file_name)

            # Load the JSON file
            with open(file_path, "r") as f:
                try:
                    file_data = json.load(f)
                except json.JSONDecodeError as e:
                    print(f"Error decoding JSON file {file_name}: {e}")
                    continue

            # Process each record in the JSON file
            if isinstance(file_data, dict):
                records = file_data.get("vulnerabilities", [])
                if not isinstance(records, list):
                    print(f"Warning: 'vulnerabilities' is not a list in file {file_name}.")
                    continue
            elif isinstance(file_data, list):
                records = file_data
            else:
                print(f"Warning: Unsupported JSON structure in file {file_name}.")
                continue

            for record in records:
                # Extract relevant fields
                snyk_id = record.get("id", "N/A")
                cve_id = record.get("CVE", "N/A")
                description = record.get("description", "N/A")
                versions = record.get("semver", {}).get("vulnerable", [])

                # Combine fields into a single text for embedding
                text = f"SNYK ID: {snyk_id}\nCVE ID: {cve_id}\nDescription: {description}\nVersions: {', '.join(versions)}"

                # Use the file name as metadata
                metadata = {"source": file_name, "snyk_id": snyk_id, "cve_id": cve_id}

                # Debugging: Print the data being embedded
                print(f"Embedding data from file: {file_name}")
                print(f"Text: {text}")
                print(f"Metadata: {metadata}")

                # Add text and metadata to the vectorstore
                try:
                    vectorstore.add_texts([text], metadatas=[metadata])
                except ValueError as e:
                    print(f"Error adding text to vectorstore for file {file_name}: {e}")

    return vectorstore
```

---

#### **Updated `main.py`**

```python
import os
import config
from langchain.llms import LlamaCpp
from langchain.embeddings import HuggingFaceEmbeddings
from database.vector_store import embed_and_store_data
from contextlib import contextmanager
import sys


@contextmanager
def suppress_stdout_stderr():
    """Context manager to suppress stdout and stderr."""
    stdout = sys.stdout
    stderr = sys.stderr
    null_device = open(os.devnull, 'w')
    try:
        sys.stdout = null_device
        sys.stderr = null_device
        yield
    finally:
        sys.stdout = stdout
        sys.stderr = stderr
        null_device.close()


def process_query(query, vectorstore, llm):
    """Process the user query and return results."""
    # Debugging: Print the query
    print(f"Processing query: {query}")

    # Search in ChromaDB
    results = vectorstore.similarity_search(query, k=5)  # Adjust `k` as needed
    if not results:
        return "No relevant data found for your query."

    # Debugging: Print the raw results
    print("Search results:")
    for result in results:
        print(f"Content: {result.page_content}")
        print(f"Metadata: {result.metadata}")

    # Extract metadata and content from results
    data = []
    for result in results:
        content = result.page_content
        metadata = result.metadata
        data.append({"content": content, "metadata": metadata})

    # Pass the content to the LLM for summarization
    summaries = summarize_data(llm, [item["content"] for item in data])

    # Combine summaries with metadata
    response = []
    for summary, item in zip(summaries, data):
        metadata = item["metadata"]
        response.append({
            "summary": summary,
            "snyk_id": metadata.get("snyk_id", "N/A"),
            "cve_id": metadata.get("cve_id", "N/A"),
            "source": metadata.get("source", "Unknown")
        })

    return response


def summarize_data(llm, data, chunk_size=512):
    """Summarize the retrieved data using the LLM."""
    summaries = []
    for content in data:
        # Split content into chunks
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]

        # Summarize each chunk
        for chunk in chunks:
            prompt = f"Summarize the following data concisely:\n\n{chunk}\n\nSummary:"
            summaries.append(llm.invoke(prompt))

    return summaries


def main():
    # Load JSON files
    print("Loading JSON files...")
    json_files = os.listdir(config.JSON_FILES_DIR)

    # Initialize the LLM
    print("Initializing the LLM...")
    with suppress_stdout_stderr():
        llm = LlamaCpp(
            model_path=config.LLM_MODEL_PATH,
            temperature=0.7,
            max_tokens=2000,
            top_p=0.9,
            verbose=True
        )

    # Initialize the embedding model
    print("Initializing the embedding model...")
    embeddings = HuggingFaceEmbeddings(
        model_name=config.EMBEDDING_MODEL_PATH,
        model_kwargs={"device": "cpu"}
    )

    # Embed and store data in ChromaDB
    print("Embedding and storing data in ChromaDB...")
    with suppress_stdout_stderr():
        vectorstore = embed_and_store_data(embeddings)

    print("Data embedding completed successfully!")

    # Interactive query loop
    print("\nReady for queries!")
    while True:
        query = input("\nEnter your query (SNYK ID, CVE ID, or keyword) or 'quit' to exit: ")
        if query.lower() == "quit":
            break

        print("\nProcessing your query...")
        response = process_query(query, vectorstore, llm)
        print("\n=== Summary ===")
        print(response)


if __name__ == "__main__":
    main()
```

---

### **Debugging Steps**

1. **Verify Embedding**:
   - Check the debug output to ensure all JSON files are being embedded into ChromaDB.
   - Verify the text and metadata being embedded.

2. **Verify Query Search**:
   - Check the debug output for the query and the search results.
   - Ensure the results are relevant to the query.

3. **Test with Different Queries**:
   - Test with `SNYK ID`, `CVE ID`, and keywords to ensure the system retrieves relevant results.

---

### **Expected Debug Output**

1. **Embedding Debug Output**:
   - For each JSON file, you should see the text and metadata being embedded.

2. **Query Debug Output**:
   - For each query, you should see the query text and the search results (content and metadata).

---

Let me know if this resolves the issue or if further adjustments are needed!
