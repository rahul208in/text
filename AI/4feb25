### **Analysis of Issues**

From the provided details and screenshots, the following two issues are identified:

---

#### **1. Missing Sentence-Transformers Model**
- **Error**: `No sentence-transformers model found with name /Users/rj22898/Documents/model/Embedded/gte-large`.
- **Cause**: The embedding model path is incorrect, or the model is not available at the specified location.
- **Fix**: Ensure the correct path to the embedding model and verify that the model exists at the specified location.

---

#### **2. `TypeError: Object of type Document is not JSON serializable`**
- **Cause**: The `Document` objects returned by ChromaDB are being passed directly to `json.dumps()`, which cannot serialize them.
- **Fix**: Extract the `page_content` or relevant fields from the `Document` objects before processing.

---

### **Proposed Fixes**

#### **1. Fix Missing Sentence-Transformers Model**
- Verify that the `gte-large` model exists at the specified path (`/Users/rj22898/Documents/model/Embedded/gte-large`).
- If the model is missing, download it using the `sentence-transformers` library:
  ```bash
  pip install -U sentence-transformers
  ```
  Then, download the `gte-large` model:
  ```python
  from sentence_transformers import SentenceTransformer
  model = SentenceTransformer('sentence-transformers/gte-large')
  model.save('/Users/rj22898/Documents/model/Embedded/gte-large')
  ```
- Update the `config.py` file to ensure the correct path:
  ```python
  EMBEDDING_MODEL_PATH = "/Users/rj22898/Documents/model/Embedded/gte-large"
  ```

---

#### **2. Fix `TypeError` for `Document` Objects**
- Extract the `page_content` or relevant fields from the `Document` objects before processing.
- Update the `summarize_large_data` function to handle this:
  ```python
  def summarize_large_data(llm, data, chunk_size=512):
      """Summarize large data in chunks."""
      # Extract content from Document objects
      combined_text = "\n".join([item.page_content for item in data])
      chunks = [combined_text[i:i+chunk_size] for i in range(0, len(combined_text), chunk_size)]
      summaries = []
      for chunk in chunks:
          prompt = f"Summarize the following data:\n\n{chunk}\n\nSummary:"
          summaries.append(llm.invoke(prompt))
      return " ".join(summaries)
  ```

---

### **Updated Code**

#### **`main.py`**
```python
import os
from langchain.llms import LlamaCpp
from langchain.embeddings import HuggingFaceEmbeddings
from database.vector_store import embed_and_store_data
import config

def summarize_large_data(llm, data, chunk_size=512):
    """Summarize large data in chunks."""
    # Extract content from Document objects
    combined_text = "\n".join([item.page_content for item in data])
    chunks = [combined_text[i:i+chunk_size] for i in range(0, len(combined_text), chunk_size)]
    summaries = []
    for chunk in chunks:
        prompt = f"Summarize the following data:\n\n{chunk}\n\nSummary:"
        summaries.append(llm.invoke(prompt))
    return " ".join(summaries)

def main():
    # Load JSON files
    print("Loading JSON files...")
    json_files = [os.path.join(config.JSON_FILES_DIR, f) for f in os.listdir(config.JSON_FILES_DIR) if f.endswith(".json")]

    # Initialize the LLM
    print("Initializing the LLM...")
    llm = LlamaCpp(model_path=config.LLM_MODEL_PATH, temperature=0.7, max_tokens=2000, top_p=0.9, verbose=True)

    # Initialize the embedding model
    print("Initializing the embedding model...")
    embeddings = HuggingFaceEmbeddings(model_name=config.EMBEDDING_MODEL_PATH, model_kwargs={"device": "cpu"})

    # Embed and store data in ChromaDB
    print("Embedding and storing data in ChromaDB...")
    vectorstore = embed_and_store_data(json_files, embeddings, config.CHROMA_DB_DIR)
    print("Data embedding completed successfully!")

    # Interactive query loop
    print("\nReady for queries!")
    while True:
        query = input("Enter your query (SNYK ID, CVE ID, or keyword) or 'quit' to exit: ")
        if query.lower() == "quit":
            break
        print("\nProcessing your query...")
        results = vectorstore.similarity_search(query, k=5)
        if results:
            summary = summarize_large_data(llm, results)
            print(f"\nSummary:\n{summary}")
        else:
            print("No relevant data found for your query.")

if __name__ == "__main__":
    main()
```

---

### **Testing Steps**

1. **Verify the Embedding Model**
   - Ensure the `gte-large` model exists at `/Users/rj22898/Documents/model/Embedded/gte-large`.
   - If missing, download and save the model as described above.

2. **Test Data Embedding**
   - Run the program and ensure all JSON files are processed without errors.
   - Check the debug output for the number of documents in ChromaDB.

3. **Test Querying**
   - Query valid CVE IDs and keywords to ensure relevant data is retrieved.

4. **Handle Large Input**
   - Test with large datasets to confirm that the input is truncated or chunked properly.

---

Let me know if you encounter further issues!
