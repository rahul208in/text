### **Analysis of the Problem**

You are facing two main issues:
1. **Exact Match Issue**: The system is returning results that are not exact matches for the query. For example, searching for `XYZ` also returns results for `XZ`, `Zimbabwe`, etc. This happens because the similarity search in ChromaDB is based on embeddings, which prioritize semantic similarity rather than exact string matching.
2. **Summary Length**: The summaries generated by the system are too long, and you want them to be shorter.

---

### **Proposed Solutions**

#### **1. Implement Exact Match Search**
To ensure exact matches, we can:
- First, check for exact matches in the JSON data before performing a similarity search.
- If an exact match is found, return it directly.
- If no exact match is found, fall back to the similarity search.

#### **2. Shorten Summaries**
To generate shorter summaries:
- Modify the prompt to instruct the LLM to generate concise summaries.
- Limit the number of tokens in the summary.

---

### **Updated Code Implementation**

#### **`main.py`**
```python
import os
from langchain.llms import LlamaCpp
from models.model_loader import CustomEmbeddings
from database.vector_store import embed_and_store_data
from utils.json_processor import load_all_json_files
import config

def exact_match_search(query, json_files):
    """Perform an exact match search in the JSON files."""
    for file in json_files:
        if isinstance(file, dict) and "vulnerabilities" in file:
            for item in file["vulnerabilities"]:
                if query in item.values():
                    return item
        elif isinstance(file, list):
            for item in file:
                if query in item.values():
                    return item
    return None

def summarize_large_data(llm, data, chunk_size=512):
    """Summarize large data in chunks."""
    combined_text = "\n".join([item.page_content for item in data])
    chunks = [combined_text[i:i+chunk_size] for i in range(0, len(combined_text), chunk_size)]
    summaries = []
    for chunk in chunks:
        prompt = f"Summarize the following data concisely:\n\n{chunk}\n\nSummary (max 50 words):"
        summaries.append(llm.invoke(prompt))
    return " ".join(summaries)

def main():
    # Load JSON files
    print("Loading JSON files...")
    json_files = load_all_json_files(config.JSON_FILES_DIR)

    # Initialize the LLM
    print("Initializing the LLM...")
    llm = LlamaCpp(model_path=config.LLM_MODEL_PATH, temperature=0.7, max_tokens=200, top_p=0.9, verbose=True)

    # Initialize the embedding model
    print("Initializing the embedding model...")
    embeddings = CustomEmbeddings(config.EMBEDDING_MODEL_PATH)

    # Embed and store data in ChromaDB
    print("Embedding and storing data in ChromaDB...")
    vectorstore = embed_and_store_data(json_files, embeddings, config.CHROMA_DB_DIR)
    print("Data embedding completed successfully!")

    # Interactive query loop
    print("\nReady for queries!")
    while True:
        query = input("Enter your query (SNYK ID, CVE ID, or keyword) or 'quit' to exit: ")
        if query.lower() == "quit":
            break

        print("\nProcessing your query...")
        # Perform exact match search
        exact_match = exact_match_search(query, json_files)
        if exact_match:
            print(f"\nExact Match Found:\n{exact_match}")
            continue

        # Perform similarity search
        results = vectorstore.similarity_search(query, k=5)
        if results:
            summary = summarize_large_data(llm, results)
            print(f"\nSummary:\n{summary}")
        else:
            print("No relevant data found for your query.")

if __name__ == "__main__":
    main()
```

---

#### **`database/vector_store.py`**
```python
from langchain_chroma import Chroma
import json

def embed_and_store_data(json_files, embeddings, persist_dir):
    """Embed and store data in ChromaDB."""
    vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    for file in json_files:
        if isinstance(file, dict) and "vulnerabilities" in file:
            # Process vulnerabilities key
            for item in file["vulnerabilities"]:
                text = json.dumps(item)  # Convert the JSON object to a string
                vectorstore.add_texts([text])
        elif isinstance(file, list):
            # Process each item in the list
            for item in file:
                text = json.dumps(item)  # Convert the JSON object to a string
                vectorstore.add_texts([text])
        else:
            # Process the entire file as a single document
            text = json.dumps(file)  # Convert the JSON object to a string
            vectorstore.add_texts([text])
    return vectorstore
```

---

#### **`models/model_loader.py`**
```python
from sentence_transformers import SentenceTransformer

class CustomEmbeddings:
    """Custom wrapper for SentenceTransformer to add embed_query and embed_documents methods."""
    def __init__(self, model_path: str):
        self.model = SentenceTransformer(model_path)

    def embed_query(self, query: str):
        return self.model.encode(query, convert_to_tensor=False).tolist()

    def embed_documents(self, documents: list):
        return [self.model.encode(doc, convert_to_tensor=False).tolist() for doc in documents]
```

---

### **Key Changes**

1. **Exact Match Search**:
   - Added the `exact_match_search` function to check for exact matches in the JSON files before performing a similarity search.

2. **Shorter Summaries**:
   - Updated the `summarize_large_data` function to generate concise summaries by modifying the prompt and limiting the token count.

3. **Improved Data Embedding**:
   - Updated the `embed_and_store_data` function to handle JSON files with varying structures.

---

### **Testing Steps**

1. **Verify Exact Match Search**:
   - Test queries with exact matches (e.g., `SNYK ID` or `CVE ID`) and verify that only exact matches are returned.

2. **Test Shorter Summaries**:
   - Query large datasets and verify that the summaries are concise and within the specified word limit.

3. **Test Data Embedding**:
   - Run the program and check that all JSON files are embedded into ChromaDB.

4. **Test Querying**:
   - Query valid SNYK IDs, CVE IDs, or keywords and verify the results.

---

Let me know if you encounter further issues!
