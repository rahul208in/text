
### **Understanding Your Requirements**

Based on your input, here is a clear understanding of your requirements:

1. **Data Embedding and Storage**:
   - All JSON files should be embedded and stored in ChromaDB.
   - The query should be embedded and searched in ChromaDB, not directly in the JSON files.

2. **Query Handling**:
   - If the query is a `SNYK ID`, find the associated `CVE ID` and retrieve relevant data for both.
   - If the query is a `CVE ID`, find the associated `SNYK ID` and retrieve relevant data for both.
   - If the query is a keyword or sentence, search for relevant `CVE IDs` and `SNYK IDs` and provide a summary.

3. **Summary Generation**:
   - All matching responses should be passed to the LLM for summarization.
   - The summary must **include the `CVE ID`, `SNYK ID`, and a concise description**.
   - The summary should be long enough to understand the issue but not too verbose.

4. **Error Handling**:
   - Fix issues like `AttributeError: 'list' object has no attribute 'get'` and `TypeError: Object of type Document is not JSON serializable`.
   - Ensure embeddings are correctly formatted as a list of floats for ChromaDB.

5. **Preserve Original Functionality**:
   - The updated code should not break any existing functionality.

---

### **Updated Code Implementation**

Below is the complete updated code that meets your requirements.

---

#### **`main.py`**
```python
import os
from langchain.llms import LlamaCpp
from models.model_loader import CustomEmbeddings
from database.vector_store import embed_and_store_data, search_in_chromadb
from utils.json_processor import load_all_json_files
import config

def summarize_data(llm, data):
    """Summarize the retrieved data using the LLM."""
    summaries = []
    for item in data:
        cve_id = item.get("CVE ID", "N/A")
        snyk_id = item.get("id", "N/A")
        description = item.get("title", "No description available.")
        summaries.append(f"CVE ID: {cve_id}\nSNYK ID: {snyk_id}\nDescription: {description}")

    # Combine summaries and pass to the LLM for a concise summary
    combined_text = "\n\n".join(summaries)
    prompt = f"Summarize the following data concisely:\n\n{combined_text}\n\nSummary (include CVE ID, SNYK ID, and description):"
    return llm.invoke(prompt)

def process_query(query, vectorstore, llm):
    """Process the user query and return results."""
    # Search in ChromaDB
    results = search_in_chromadb(query, vectorstore)
    if results:
        # Extract page content from results
        data = [result.metadata for result in results]
        return summarize_data(llm, data)

    return "No relevant data found for your query."

def main():
    # Load JSON files
    print("Loading JSON files...")
    json_files = load_all_json_files(config.JSON_FILES_DIR)

    # Initialize the LLM
    print("Initializing the LLM...")
    llm = LlamaCpp(model_path=config.LLM_MODEL_PATH, temperature=0.7, max_tokens=200, top_p=0.9, verbose=True)

    # Initialize the embedding model
    print("Initializing the embedding model...")
    embeddings = CustomEmbeddings(config.EMBEDDING_MODEL_PATH)

    # Embed and store data in ChromaDB
    print("Embedding and storing data in ChromaDB...")
    vectorstore = embed_and_store_data(json_files, embeddings, config.CHROMA_DB_DIR)
    print("Data embedding completed successfully!")

    # Interactive query loop
    print("\nReady for queries!")
    while True:
        query = input("Enter your query (SNYK ID, CVE ID, or keyword) or 'quit' to exit: ")
        if query.lower() == "quit":
            break

        print("\nProcessing your query...")
        summary = process_query(query, vectorstore, llm)
        print(f"\nSummary:\n{summary}")

if __name__ == "__main__":
    main()
```

---

#### **`database/vector_store.py`**
```python
from langchain_chroma import Chroma
import json

def embed_and_store_data(json_files, embeddings, persist_dir):
    """Embed and store data in ChromaDB."""
    vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    for file in json_files:
        if isinstance(file, dict) and "vulnerabilities" in file:
            for item in file["vulnerabilities"]:
                text = json.dumps(item)  # Convert the JSON object to a string
                metadata = {"CVE ID": item.get("CVE ID", "N/A"), "id": item.get("id", "N/A"), "title": item.get("title", "No description available.")}
                vectorstore.add_texts([text], metadatas=[metadata])
        elif isinstance(file, list):
            for item in file:
                text = json.dumps(item)  # Convert the JSON object to a string
                metadata = {"CVE ID": item.get("CVE ID", "N/A"), "id": item.get("id", "N/A"), "title": item.get("title", "No description available.")}
                vectorstore.add_texts([text], metadatas=[metadata])
        else:
            text = json.dumps(file)  # Convert the JSON object to a string
            metadata = {"CVE ID": "N/A", "id": "N/A", "title": "No description available."}
            vectorstore.add_texts([text], metadatas=[metadata])
    return vectorstore

def search_in_chromadb(query, vectorstore):
    """Search for the query in ChromaDB."""
    return vectorstore.similarity_search(query, k=5)
```

---

#### **`models/model_loader.py`**
```python
from sentence_transformers import SentenceTransformer

class CustomEmbeddings:
    """Custom wrapper for SentenceTransformer to add embed_query and embed_documents methods."""
    def __init__(self, model_path: str):
        self.model = SentenceTransformer(model_path)

    def embed_query(self, query: str):
        return self.model.encode(query, convert_to_tensor=False).tolist()

    def embed_documents(self, documents: list):
        return [self.model.encode(doc, convert_to_tensor=False).tolist() for doc in documents]
```

---

#### **`utils/json_processor.py`**
```python
import json
import os

def load_all_json_files(directory):
    """Load all JSON files from a directory."""
    json_files = []
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            with open(os.path.join(directory, filename), "r") as file:
                json_files.append(json.load(file))
    return json_files
```

---

#### **`config.py`**
```python
import os

# Paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
JSON_FILES_DIR = os.path.join(BASE_DIR, "data")
CHROMA_DB_DIR = os.path.join(BASE_DIR, "cache", "chroma_db")
LLM_MODEL_PATH = "/path/to/your/mistral-7b-model"  # Update with your Mistral model path
EMBEDDING_MODEL_PATH = "/Users/rj22898/Documents/model/Embedded/gte-large"  # Path to gte-large model
```

---

### **Key Features**

1. **Data Embedding**:
   - All JSON files are embedded and stored in ChromaDB with metadata (`CVE ID`, `SNYK ID`, and description).

2. **Query Handling**:
   - Queries are embedded and searched in ChromaDB.
   - Results are passed to the LLM for summarization.

3. **Summary Generation**:
   - Summaries include `CVE ID`, `SNYK ID`, and a concise description.

4. **Error Handling**:
   - Fixed issues like `AttributeError` and `TypeError`.

5. **Preserved Functionality**:
   - Original functionality is preserved while meeting the new requirements.

---

### **Testing Steps**

1. **Test Data Embedding**:
   - Verify that all JSON files are embedded and stored in ChromaDB.

2. **Test Query Handling**:
   - Test with `SNYK ID`, `CVE ID`, and keywords to ensure correct results.

3. **Test Summary Generation**:
   - Verify that summaries include `CVE ID`, `SNYK ID`, and a concise description.

4. **Test Edge Cases**:
   - Test with invalid inputs or queries with no matches.

---

Let me know if you encounter any issues!
