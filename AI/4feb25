
Thank you for sharing the details and screenshots. Based on the information provided, I can see that:

1. Your JSON files are located in the `data` folder.
2. The `vector_store.py` script is responsible for embedding and storing the data.
3. The `config.py` file contains paths and settings for the project, including the `JSON_FILES_DIR` and `EMBEDDING_MODEL_PATH`.

I'll now provide an updated version of your `vector_store.py` script to:
- Automatically read all JSON files from the `data` folder.
- Use the Hugging Face model for embedding.
- Store the embeddings in the Chroma vector store.

---

### **Updated `vector_store.py`**
```python
import os
import json
from typing import List, Any
import torch
from langchain.vectorstores import Chroma
from transformers import AutoModel, AutoTokenizer
from config import JSON_FILES_DIR, CHROMA_DB_DIR, EMBEDDING_MODEL_PATH


def embed_and_store_data(data_folder: str, persist_dir: str, model_path: str):
    """
    Reads all JSON files from the specified folder, embeds their content using the Hugging Face model,
    and stores the embeddings in a Chroma vector store.

    Args:
        data_folder (str): Path to the folder containing JSON files.
        persist_dir (str): Path to the directory where the vector store will be persisted.
        model_path (str): Path to the Hugging Face model.

    Returns:
        Chroma: The vector store containing the embedded data.
    """
    # Load the Hugging Face model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModel.from_pretrained(model_path)

    # Initialize the Chroma vector store
    vectorstore = Chroma(persist_directory=persist_dir, embedding_function=None)

    # Get all JSON files from the data folder
    json_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith(".json")]

    for file_path in json_files:
        print(f"Processing file: {file_path}")
        try:
            with open(file_path, "r") as f:
                data = json.load(f)

            # Extract text data from the JSON file
            text_corpus = []
            if isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        text_corpus.append(json.dumps(item))
                    elif isinstance(item, str):
                        text_corpus.append(item)
            elif isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, str):
                        text_corpus.append(value)
                    elif isinstance(value, list):
                        for sub_item in value:
                            if isinstance(sub_item, str):
                                text_corpus.append(sub_item)
                            elif isinstance(sub_item, dict):
                                text_corpus.append(json.dumps(sub_item))
            else:
                print(f"Skipping unsupported structure in file: {file_path}")
                continue

            # Embed the text data
            embeddings = []
            for text in text_corpus:
                inputs = tokenizer(text, padding=True, truncation=True, return_tensors="pt")
                with torch.no_grad():
                    outputs = model(**inputs)
                # Use mean pooling for embeddings
                attention_mask = inputs['attention_mask']
                mean_embedding = (outputs.last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1).unsqueeze(-1)
                embeddings.append(mean_embedding.squeeze().numpy())

            # Store the embeddings in the vector store
            for text, embedding in zip(text_corpus, embeddings):
                vectorstore.add_texts([text], [embedding])

        except json.JSONDecodeError as e:
            print(f"Error decoding JSON in file {file_path}: {e}")

    # Persist the vector store
    vectorstore.persist()
    print(f"Vector store persisted in: {persist_dir}")

    return vectorstore


if __name__ == "__main__":
    # Use paths from the config file
    embed_and_store_data(JSON_FILES_DIR, CHROMA_DB_DIR, EMBEDDING_MODEL_PATH)
```

---

### **Key Updates**
1. **Dynamic File Loading**:
   - The script automatically reads all `.json` files from the `data` folder (`JSON_FILES_DIR` from `config.py`).

2. **Embedding Logic**:
   - The Hugging Face model is used to tokenize and embed the text.
   - Mean pooling is applied to generate embeddings.

3. **Chroma Vector Store**:
   - The embeddings are stored in the Chroma vector store, and the store is persisted in the directory specified by `CHROMA_DB_DIR`.

4. **Integration with `config.py`**:
   - The script uses paths and settings from the `config.py` file for better modularity.

---

### **Usage**
1. **Run the Script**:
   - Ensure the `config.py` file is correctly configured with the paths to your `data` folder, Chroma DB directory, and model path.
   - Run the script:
     ```bash
     python vector_store.py
     ```

2. **Expected Output**:
   -
