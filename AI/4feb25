
### **Analysis of Issues**

From the provided screenshots and error messages, the following issues are identified:

1. **`AttributeError: 'list' object has no attribute 'get'`**
   - This occurs because the `embed_and_store_data` function assumes that all JSON files have a `vulnerabilities` key at the top level. However, not all JSON files follow this structure.

2. **`TypeError: Object of type Document is not JSON serializable`**
   - This happens when trying to serialize `Document` objects directly. The `page_content` or relevant fields need to be extracted before serialization.

3. **`ValueError: Requested tokens exceed context window`**
   - The input text being passed to the LLM exceeds the context window size. This needs to be chunked into smaller parts.

4. **Deprecation Warning for `Chroma`**
   - The `Chroma` class is deprecated in `langchain`. The updated `langchain_chroma` package should be used instead.

---

### **Proposed Fixes**

#### **1. Handle JSON Files with Different Structures**
- Update the `embed_and_store_data` function to handle JSON files with varying structures. For example:
  - If the file contains a `vulnerabilities` key, process it.
  - Otherwise, process the entire file as a single document.

#### **2. Extract `page_content` from `Document` Objects**
- Update the `summarize_large_data` function to extract the `page_content` field from `Document` objects.

#### **3. Chunk Large Inputs**
- Implement a chunking mechanism to split large inputs into smaller parts that fit within the LLM's context window.

#### **4. Use Updated `langchain_chroma` Package**
- Replace the deprecated `Chroma` class with the updated version.

---

### **Complete Code Implementation**

#### **`database/vector_store.py`**
```python
from langchain_chroma import Chroma
import json

def embed_and_store_data(json_files, embeddings, persist_dir):
    """Embed and store data in ChromaDB."""
    vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    for file in json_files:
        if isinstance(file, dict) and "vulnerabilities" in file:
            # Process vulnerabilities key
            for item in file["vulnerabilities"]:
                text = json.dumps(item)  # Convert the JSON object to a string
                vectorstore.add_texts([text])
        else:
            # Process the entire file as a single document
            text = json.dumps(file)  # Convert the JSON object to a string
            vectorstore.add_texts([text])
    return vectorstore
```

---

#### **`main.py`**
```python
import os
from langchain.llms import LlamaCpp
from models.model_loader import CustomEmbeddings
from database.vector_store import embed_and_store_data
from utils.json_processor import load_all_json_files
import config

def summarize_large_data(llm, data, chunk_size=512):
    """Summarize large data in chunks."""
    combined_text = "\n".join([item.page_content for item in data])
    chunks = [combined_text[i:i+chunk_size] for i in range(0, len(combined_text), chunk_size)]
    summaries = []
    for chunk in chunks:
        prompt = f"Summarize the following data:\n\n{chunk}\n\nSummary:"
        summaries.append(llm.invoke(prompt))
    return " ".join(summaries)

def main():
    # Load JSON files
    print("Loading JSON files...")
    json_files = load_all_json_files(config.JSON_FILES_DIR)

    # Initialize the LLM
    print("Initializing the LLM...")
    llm = LlamaCpp(model_path=config.LLM_MODEL_PATH, temperature=0.7, max_tokens=2000, top_p=0.9, verbose=True)

    # Initialize the embedding model
    print("Initializing the embedding model...")
    embeddings = CustomEmbeddings(config.EMBEDDING_MODEL_PATH)

    # Embed and store data in ChromaDB
    print("Embedding and storing data in ChromaDB...")
    vectorstore = embed_and_store_data(json_files, embeddings, config.CHROMA_DB_DIR)
    print("Data embedding completed successfully!")

    # Interactive query loop
    print("\nReady for queries!")
    while True:
        query = input("Enter your query (SNYK ID, CVE ID, or keyword) or 'quit' to exit: ")
        if query.lower() == "quit":
            break
        print("\nProcessing your query...")
        results = vectorstore.similarity_search(query, k=5)
        if results:
            summary = summarize_large_data(llm, results)
            print(f"\nSummary:\n{summary}")
        else:
            print("No relevant data found for your query.")

if __name__ == "__main__":
    main()
```

---

#### **`utils/json_processor.py`**
```python
import json
import os

def load_all_json_files(directory):
    """Load all JSON files from a directory."""
    json_files = []
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            with open(os.path.join(directory, filename), "r") as file:
                json_files.append(json.load(file))
    return json_files
```

---

#### **`models/model_loader.py`**
```python
from sentence_transformers import SentenceTransformer

class CustomEmbeddings:
    """Custom wrapper for SentenceTransformer to add embed_query
