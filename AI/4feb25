
### **Understanding Your Problem**

1. **Issue 1: "No relevant data found for your query"**
   - You have around 70,000 JSON files, and you're unsure if they are being properly loaded into ChromaDB.
   - When you query a valid CVE ID, the system responds with "No relevant data found for your query."

2. **Issue 2: Handling Large Input**
   - You are encountering a `ValueError: Requested tokens exceed context window` when processing large inputs for the LLM.

---

### **Proposed Solutions**

#### **Issue 1: Ensuring JSON Files Are Loaded into ChromaDB**

##### **Steps to Debug and Fix**

1. **Verify JSON File Loading**
   - Ensure that all JSON files are being read and processed correctly.
   - Add a debug statement to print the filenames being processed.

   Example:
   ```python
   for file_path in json_files:
       print(f"Processing file: {file_path}")
   ```

2. **Handle JSON Structure**
   - The error `AttributeError: 'list' object has no attribute 'get'` indicates that some JSON files are lists instead of dictionaries.
   - Update the `embed_and_store_data` function to handle both cases.

   Updated Code:
   ```python
   def embed_and_store_data(json_files, embeddings, persist_dir):
       vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
       for file_path in json_files:
           with open(file_path, "r") as f:
               data = json.load(f)
               if isinstance(data, dict):
                   for item in data.get("vulnerabilities", []):
                       text = json.dumps(item)
                       vectorstore.add_texts([text])
               elif isinstance(data, list):
                   for item in data:
                       text = json.dumps(item)
                       vectorstore.add_texts([text])
       return vectorstore
   ```

3. **Check ChromaDB Content**
   - After embedding, verify that the data is stored in ChromaDB.
   - Add a debug statement to print the number of documents in the vector store.

   Example:
   ```python
   print(f"Number of documents in ChromaDB: {len(vectorstore.get())}")
   ```

4. **Test Querying**
   - Test querying ChromaDB directly to ensure the data is retrievable.
   - Example:
     ```python
     results = vectorstore.similarity_search("CVE-2024-3144", k=5)
     print(f"Query results: {results}")
     ```

---

#### **Issue 2: Handling Large Input for LLM**

##### **Steps to Fix**

1. **Truncate Input**
   - Limit the input text passed to the LLM to fit within its context window.
   - Example:
     ```python
     max_tokens = 512  # Adjust based on your LLM's context window
     truncated_text = combined_text[:max_tokens]
     ```

2. **Summarize in Chunks**
   - If the input is too large, split it into smaller chunks and summarize each chunk separately.
   - Example:
     ```python
     def summarize_large_data(llm, data, chunk_size=512):
         combined_text = "\n".join([json.dumps(item, indent=2) for item in data])
         chunks = [combined_text[i:i+chunk_size] for i in range(0, len(combined_text), chunk_size)]
         summaries = []
         for chunk in chunks:
             prompt = f"Summarize the following data:\n\n{chunk}\n\nSummary:"
             summaries.append(llm.invoke(prompt))
         return " ".join(summaries)
     ```

3. **Optimize Prompt**
   - Use a concise prompt to reduce token usage.
   - Example:
     ```python
     summarization_prompt = f"Summarize:\n{truncated_text}"
     ```

---

### **Updated Code**

#### **`vector_store.py`**
```python
import os
import json
from langchain_chroma import Chroma

def embed_and_store_data(json_files, embeddings, persist_dir):
    """Embed and store data in ChromaDB."""
    vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    for file_path in json_files:
        print(f"Processing file: {file_path}")
        with open(file_path, "r") as f:
            data = json.load(f)
            if isinstance(data, dict):
                for item in data.get("vulnerabilities", []):
                    text = json.dumps(item)
                    vectorstore.add_texts([text])
            elif isinstance(data, list):
                for item in data:
                    text = json.dumps(item)
                    vectorstore.add_texts([text])
    print(f"Number of documents in ChromaDB: {len(vectorstore.get())}")
    return vectorstore
```

---

#### **`main.py`**
```python
import os
from langchain.llms import LlamaCpp
from langchain.embeddings import HuggingFaceEmbeddings
from database.vector_store import embed_and_store_data
import config

def summarize_large_data(llm, data, chunk_size=512):
    """Summarize large data in chunks."""
    combined_text = "\n".join([json.dumps(item, indent=2) for item in data])
    chunks = [combined_text[i:i+chunk_size] for i in range(0, len(combined_text), chunk_size)]
    summaries = []
    for chunk in chunks:
        prompt = f"Summarize the following data:\n\n{chunk}\n\nSummary:"
        summaries.append(llm.invoke(prompt))
    return " ".join(summaries)

def main():
    # Load JSON files
    json_files = [os.path.join(config.JSON_FILES_DIR, f) for f in os.listdir(config.JSON_FILES_DIR) if f.endswith(".json")]

    # Initialize the LLM
    llm = LlamaCpp(model_path=config.LLM_MODEL_PATH, temperature=0.7, max_tokens=2000, top_p=0.9, verbose=True)

    # Initialize the embedding model
    embeddings = HuggingFaceEmbeddings(model_name=config.EMBEDDING_MODEL_PATH, model_kwargs={"device": "cpu"})

    # Embed and store data in ChromaDB
    vectorstore = embed_and_store_data(json_files, embeddings, config.CHROMA_DB_DIR)

    # Interactive query loop
    while True:
        query = input("Enter your query (CVE ID or keyword) or 'quit' to exit: ")
        if query.lower() == "quit":
            break
        results = vectorstore.similarity_search(query, k=5)
        if results:
            summary = summarize_large_data(llm, results)
            print(f"Summary: {summary}")
        else:
            print("No relevant data found for your query.")

if __name__ == "__main__":
    main()
```

---

### **Testing Steps**

1. **Verify Data Loading**
   - Run the program and ensure all JSON files are processed without errors.
   - Check the debug output for the number of documents in ChromaDB.

2. **Test Querying**
   - Query valid CVE IDs and keywords to ensure relevant data is retrieved.

3. **Handle Large Input**
   - Test with large datasets to confirm that the input is truncated or chunked properly.

---

Let me know if you encounter further issues!
