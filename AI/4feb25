
### Analysis of Issues

From the provided images and error messages, the following issues are identified:

1. **`AttributeError: 'list' object has no attribute 'get'`**:
   - This occurs because the code assumes that all JSON files are dictionaries with a `vulnerabilities` key. However, some JSON files are lists or have different structures.

2. **Deprecation Warning for `Chroma`**:
   - The `Chroma` class is deprecated in `langchain` and should be imported from `langchain_chroma`.

3. **Query Processing Not Implemented**:
   - The query processing logic is incomplete in the `main.py` file.

4. **`ValueError: Requested tokens exceed context window`**:
   - The LLM is being passed a prompt that exceeds its maximum token limit.

---

### Updated Code

Below is the complete updated code to address all the issues.

---

#### **1. `config.py`**
```python
# config.py

# Directory containing JSON files
JSON_FILES_DIR = "data"

# Path to the Mistral 7B LLM model
LLM_MODEL_PATH = "./models/mistral-7b"

# Path to the embedding model
EMBEDDING_MODEL_PATH = "sentence-transformers/all-MiniLM-L6-v2"

# Path to the ChromaDB persistence directory
CHROMA_DB_DIR = "./cache/chroma_db"
```

---

#### **2. `utils/json_processor.py`**
```python
import os
import json
from typing import List, Dict, Any


def load_all_json_files(directory: str) -> List[Any]:
    """Load all JSON files from a directory."""
    json_data = []
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            with open(os.path.join(directory, filename), "r") as f:
                data = json.load(f)
                json_data.append(data)
    return json_data


def find_data_by_keyword(keyword: str, json_files: List[Any]) -> List[Dict[str, Any]]:
    """Search for any keyword in all JSON files."""
    results = []
    for file in json_files:
        if isinstance(file, dict):
            for item in file.get("vulnerabilities", []):
                if keyword.lower() in json.dumps(item).lower():
                    results.append(item)
        elif isinstance(file, list):
            for item in file:
                if keyword.lower() in json.dumps(item).lower():
                    results.append(item)
    return results


def find_cve_from_snyk(snyk_id: str, json_files: List[Any]) -> List[str]:
    """Find CVE(s) associated with a given SNYK ID."""
    cve_ids = []
    for file in json_files:
        if isinstance(file, dict):
            for item in file.get("vulnerabilities", []):
                if item.get("id") == snyk_id:
                    cve_ids.extend(item.get("identifiers", {}).get("CVE", []))
    return cve_ids
```

---

#### **3. `database/vector_store.py`**
```python
from langchain_chroma import Chroma
import json
from typing import List, Any


def embed_and_store_data(json_files: List[Any], embeddings, persist_dir: str) -> Chroma:
    """Embed and store data in ChromaDB."""
    vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)

    for file in json_files:
        if isinstance(file, dict):
            for item in file.get("vulnerabilities", []):
                text = json.dumps(item)  # Convert the JSON object to a string
                vectorstore.add_texts([text])
        elif isinstance(file, list):
            for item in file:
                text = json.dumps(item)  # Convert the JSON object to a string
                vectorstore.add_texts([text])

    return vectorstore
```

---

#### **4. `main.py`**
```python
import sys
import os
import contextlib
from langchain.llms import LlamaCpp
from langchain.embeddings import HuggingFaceEmbeddings
from utils.json_processor import load_all_json_files, find_data_by_keyword, find_cve_from_snyk
from database.vector_store import embed_and_store_data
import config


@contextlib.contextmanager
def suppress_stdout_stderr():
    """Context manager to suppress stdout and stderr."""
    stdout = sys.stdout
    stderr = sys.stderr
    null_device = open(os.devnull, 'w')
    try:
        sys.stdout = null_device
        sys.stderr = null_device
        yield
    finally:
        sys.stdout = stdout
        sys.stderr = stderr
        null_device.close()


def summarize_data(llm, data):
    """Summarize the given data using an LLM."""
    combined_text = "\n".join([json.dumps(item, indent=2) for item in data])
    # Truncate the text to fit within the LLM's context window
    max_tokens = 512  # Adjust based on your LLM's context window
    truncated_text = combined_text[:max_tokens]
    summarization_prompt = f"Summarize the following data:\n\n{truncated_text}\n\nSummary:"
    return llm.invoke(summarization_prompt)


def process_query(query, vectorstore, json_files, llm):
    """Process a query and return a summarized response."""
    results = []

    # Check if the query is a SNYK ID
    cve_ids = find_cve_from_snyk(query, json_files)
    if cve_ids:
        for cve_id in cve_ids:
            # Search for CVE data in JSON files
            cve_results = find_data_by_keyword(cve_id, json_files)
            results.extend(cve_results)

    # If no CVE IDs found, treat the query as a general keyword
    if not results:
        results = find_data_by_keyword(query, json_files)

    # Summarize the results
    if results:
        summary = summarize_data(llm, results)
        return summary
    else:
        return "No relevant data found for your query."


def main():
    # Load JSON files
    print("Loading JSON files...")
    json_files = load_all_json_files(config.JSON_FILES_DIR)

    # Initialize the LLM
    print("Initializing the LLM...")
    with suppress_stdout_stderr():
        llm = LlamaCpp(
            model_path=config.LLM_MODEL_PATH,
            temperature=0.7,
            max_tokens=2000,
            top_p=0.9,
            verbose=True
        )

    # Initialize the embedding model
    print("Initializing the embedding model...")
    embeddings = HuggingFaceEmbeddings(
        model_name=config.EMBEDDING_MODEL_PATH,
        model_kwargs={'device': 'cpu'}
    )

    # Embed and store data in ChromaDB
    print("Embedding and storing data in ChromaDB...")
    with suppress_stdout_stderr():
        vectorstore = embed_and_store_data(json_files, embeddings, config.CHROMA_DB_DIR)

    print("Data embedding completed successfully!")

    # Interactive query loop
    print("\nReady for queries!")
    while True:
        query = input("\nEnter your query (SNYK ID, CVE ID, or keyword) or 'quit' to exit: ")
        if query.lower() == "quit":
            break

        print("\nProcessing your query...")
        response = process_query(query, vectorstore, json_files, llm)
        print("\n=== Summary ===")
        print(response)


if __name__ == "__main__":
    main()
```

---

### Key Fixes

1. **`AttributeError` Fix**:
   - The `embed_and_store_data` function now handles both dictionaries and lists in JSON files.

2. **Deprecation Warning**:
   - The `Chroma` class is imported from `langchain_chroma`.

3. **Query Processing**:
   - The `process_query` function is implemented to handle SNYK IDs, CVE IDs, and keywords.

4. **Token Limit**:
   - The `summarize_data` function truncates the input text to fit within the LLM's context window.

---

### Testing

1. **Test with JSON Files**:
   - Place JSON files with and without `vulnerabilities` in the `data/` directory.
   - Run the script and test queries for SNYK IDs, CVE IDs, and keywords.

2. **Expected Output**:
   - For valid queries, a summarized response is returned.
   - For invalid queries, the system responds with: `"No relevant data found for your query."`

Let me know if you encounter any further issues!
