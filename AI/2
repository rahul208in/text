
The folder you have for the `gte-large` model contains all the necessary files to load the model locally. These files are typical for Hugging Face models and include:

1. **`config.json`**: Configuration file for the model architecture.
2. **`pytorch_model.bin`**: The actual model weights.
3. **`tokenizer_config.json`**: Configuration for the tokenizer.
4. **`vocab.txt`**: Vocabulary file for the tokenizer.
5. **`sentence_bert_config.json`**: Configuration specific to Sentence-BERT models.
6. **Other files**: Supporting files like `special_tokens_map.json`, `data.pkl`, etc.

You can load this model locally using the `SentenceTransformer` class from the `sentence-transformers` library. Here's how you can do it:

---

### **Steps to Use the Local GTE-Large Model**

1. **Ensure the `sentence-transformers` library is installed**:
   If it's not already installed in your environment, ensure it's available in your intranet artifactory and install it:
   ```bash
   pip install sentence-transformers
   ```

2. **Update the Path to Your Local Model**:
   Use the path to the folder containing the files (e.g., `/path/to/gte-large`).

3. **Load the Model Locally**:
   Modify your embedding handler to load the model from the local folder.

---

### **Code to Load and Use the Local GTE-Large Model**

Here’s the updated `GTEEmbeddingHandler` class:

```python
from sentence_transformers import SentenceTransformer
from typing import List, Dict
from pathlib import Path
from loguru import logger
from langchain.schema import Document

class GTEEmbeddingHandler:
    """
    Handles document embedding using GTE-Large model locally
    """
    def __init__(self,
                 model_path: str = "/path/to/gte-large",  # Update this path
                 cache_dir: str = "cache/embeddings"):
        """
        Initialize the GTEEmbeddingHandler.

        Args:
            model_path (str): Path to the local GTE-Large model files
            cache_dir (str): Directory to store embedding cache
        """
        try:
            self.model = SentenceTransformer(model_path)
            self.cache_dir = Path(cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Initialized GTE-Large embedding model from: {model_path}")
        except Exception as e:
            logger.error(f"Error loading model: {str(e)}")
            raise

    def get_embedding(self, text: str) -> List[float]:
        """
        Get embedding for a text using GTE-Large
        """
        try:
            embedding = self.model.encode(text)
            return embedding.tolist()
        except Exception as e:
            logger.error(f"Error getting embedding: {str(e)}")
            raise

    def embed_documents(self, documents: List[Document]) -> List[Dict]:
        """
        Embed a list of documents.
        """
        embedded_documents = []
        for doc in documents:
            try:
                embedding = self.get_embedding(doc.page_content)
                embedded_doc = {
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "embedding": embedding
                }
                embedded_documents.append(embedded_doc)
                logger.info(f"Successfully embedded document: {doc.metadata.get('file_name')}")
            except Exception as e:
                logger.error(f"Error embedding document {doc.metadata.get('file_name')}: {str(e)}")

        return embedded_documents
```

---

### **Test Script**

Here’s how you can test the embedding handler with your local GTE-Large model:

```python
from app.ingestion.ingestion import DataIngestion
from app.embedding.gte_embedding import GTEEmbeddingHandler

def test_gte_embedding():
    # Initialize DataIngestion
    folder_path = "data/json_files/"
    ingestion = DataIngestion(folder_path)

    # Load documents
    documents = ingestion.load_json_files()

    # Initialize GTEEmbeddingHandler
    embedding_handler = GTEEmbeddingHandler(model_path="/path/to/gte-large")  # Update the path

    # Embed documents
    embedded_docs = embedding_handler.embed_documents(documents)

    # Print results
    for doc in embedded_docs:
        print(f"File: {doc['metadata']['file_name']}")
        print(f"Embedding length: {len(doc['embedding'])}")
        print(f"First few dimensions: {doc['embedding'][:5]}")  # Print first 5 dimensions
        print("-" * 50)

if __name__ == "__main__":
    test_gte_embedding()
```

---

### **Expected Output**

For each document, you should see:
- The file name
- The length of the embedding vector (should be 1024 for GTE-Large)
- The first few dimensions of the embedding vector

Example:
```
File: file1.json
Embedding length: 1024
First few dimensions: [0.123, -0.456, 0.789, -0.101, 0.112]
--------------------------------------------------
File: file2.json
Embedding length: 1024
First few dimensions: [0.234, -0.567, 0.890, -0.202, 0.223]
--------------------------------------------------
```

---

### **Key Points**
1. Ensure the `model_path` points to the folder containing the GTE-Large files.
2. The `SentenceTransformer` class will automatically load the model from the local folder.
3. The embeddings generated will be 1024-dimensional vectors.

Let me know if you encounter any issues or need further assistance!
