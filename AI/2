Let's implement the SentenceTransformers approach for local embeddings. First, install the required package:

```bash
pip install sentence-transformers
```

Here's the updated embedding handler implementation:

```python
# app/embedding/local_embedding.py

import torch
from pathlib import Path
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from loguru import logger
from langchain.schema import Document
import pickle
import hashlib

class LocalEmbeddingHandler:
    """
    Handles document embedding using SentenceTransformers locally
    """
    def __init__(self, 
                 model_name: str = 'all-MiniLM-L6-v2',
                 cache_dir: str = "cache/embeddings"):
        """
        Initialize the LocalEmbeddingHandler.

        Args:
            model_name (str): Name of the SentenceTransformer model to use
            cache_dir (str): Directory to store embedding cache
        """
        self.model = SentenceTransformer(model_name)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Initialized local embedding model: {model_name}")
        
    def _get_cache_key(self, text: str) -> str:
        """Generate a cache key for a given text"""
        return hashlib.sha256(text.encode()).hexdigest()

    def _get_from_cache(self, cache_key: str) -> List[float] | None:
        """Retrieve embedding from cache if it exists"""
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.error(f"Error loading cache: {str(e)}")
                return None
        return None

    def _save_to_cache(self, cache_key: str, embedding: List[float]) -> None:
        """Save embedding to cache"""
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(embedding, f)
        except Exception as e:
            logger.error(f"Error saving to cache: {str(e)}")

    def get_embedding(self, text: str) -> List[float]:
        """
        Get embedding for a text, using cache if available.

        Args:
            text (str): Text to get embedding for

        Returns:
            List[float]: Embedding vector
        """
        cache_key = self._get_cache_key(text)
        cached_embedding = self._get_from_cache(cache_key)

        if cached_embedding is not None:
            logger.info("Using cached embedding")
            return cached_embedding

        try:
            # Convert text to embedding using SentenceTransformer
            embedding = self.model.encode(text)
            embedding_list = embedding.tolist()
            
            # Save to cache
            self._save_to_cache(cache_key, embedding_list)
            
            return embedding_list
        except Exception as e:
            logger.error(f"Error getting embedding: {str(e)}")
            raise

    def embed_documents(self, documents: List[Document]) -> List[Dict]:
        """
        Embed a list of documents.

        Args:
            documents (List[Document]): Documents to embed

        Returns:
            List[Dict]: List of documents with embeddings
        """
        embedded_documents = []

        for doc in documents:
            try:
                embedding = self.get_embedding(doc.page_content)
                embedded_doc = {
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "embedding": embedding
                }
                embedded_documents.append(embedded_doc)
                logger.info(f"Successfully embedded document: {doc.metadata.get('file_name')}")
            except Exception as e:
                logger.error(f"Error embedding document {doc.metadata.get('file_name')}: {str(e)}")

        return embedded_documents
```

Create a test script `test_local_embedding.py`:

```python
from app.ingestion.ingestion import DataIngestion
from app.embedding.local_embedding import LocalEmbeddingHandler

def test_embedding():
    # Initialize DataIngestion
    folder_path = "data/json_files/"
    ingestion = DataIngestion(folder_path)

    # Load documents
    documents = ingestion.load_json_files()

    # Initialize LocalEmbeddingHandler
    embedding_handler = LocalEmbeddingHandler()

    # Embed documents
    embedded_docs = embedding_handler.embed_documents(documents)

    # Print results
    for doc in embedded_docs:
        print(f"File: {doc['metadata']['file_name']}")
        print(f"Embedding length: {len(doc['embedding'])}")
        print("-" * 50)

if __name__ == "__main__":
    test_embedding()
```

Key features of this implementation:

1. **Local Processing**: All embedding is done locally using SentenceTransformers
2. **Caching**: Implements local caching to avoid re-embedding identical content
3. **Error Handling**: Includes robust error handling and logging
4. **Memory Efficient**: Converts torch tensors to lists for storage
5. **Easy to Use**: Simple interface that maintains compatibility with the previous implementation

To test:
```bash
python test_local_embedding.py
```

The `all-MiniLM-L6-v2` model will be downloaded automatically on first use (~100MB) and cached locally. This model provides a good balance between speed and quality for most use cases.

Would you like to proceed with testing this implementation, or would you like me to explain any part in more detail?
