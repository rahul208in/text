import os
from llama_cpp import Llama

# ========= CONFIG ==========
MODEL_PATH = "path/to/llama2-7b-nous-hermes-8-bit.gguf"
LOG_FILE_PATH = "logs/performance_test.log"
CHUNK_SIZE = 5000  # characters per chunk
# ===========================

# Initialize model
llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=4096,
    n_threads=4
)

def read_and_chunk_logs(file_path, chunk_size=5000):
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
    return [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]

def filter_relevant_logs(log_chunk):
    """Step 1: Extract only useful parts from a log chunk using LLM."""
    prompt = f"""
You are a log analysis assistant.
From the following log chunk, extract ONLY relevant lines that indicate:
- Errors, Exceptions, Failures
- Timeouts, Slow queries or slow APIs
- High response time, memory leaks, GC pauses
- AppDynamics/Splunk alerts, CPU/Memory spikes

Remove all INFO/debug/noise logs unless needed for context.

Logs:
{log_chunk}

Return only filtered relevant logs.
"""
    response = llm(prompt, max_tokens=1024, temperature=0.2)
    return response["choices"][0]["text"].strip()

def generate_final_analysis(filtered_logs):
    """Step 2: Analyze filtered logs and produce final insights."""
    prompt = f"""
You are an expert performance engineer and SRE.

Analyze the following filtered logs from performance tests, Splunk and AppDynamics:

{filtered_logs}

Provide a structured output:
1. Overall Summary â€“ Test passed/failed, stability, main issues.
2. Failure Reasons â€“ Root cause analysis for any errors.
3. Performance Bottlenecks â€“ Slow APIs, DB queries, thread blocking, CPU/memory issues.
4. Optimization Recommendations â€“ What should be improved and how.
5. Key Observations.

Make it clear, concise, and insightful.
"""
    response = llm(prompt, max_tokens=2048, temperature=0.3)
    return response["choices"][0]["text"].strip()

if __name__ == "__main__":
    print("ðŸ”¹ Step 1: Reading and chunking logs...")
    chunks = read_and_chunk_logs(LOG_FILE_PATH, CHUNK_SIZE)

    filtered_results = []
    print(f"ðŸ”¹ Step 2: Filtering {len(chunks)} log chunks using LLM...\n")
    for i, chunk in enumerate(chunks):
        print(f"   âœ… Filtering chunk {i+1}/{len(chunks)}")
        filtered = filter_relevant_logs(chunk)
        filtered_results.append(filtered)

    combined_filtered_logs = "\n".join(filtered_results)

    print("\nðŸ”¹ Step 3: Analyzing filtered logs with LLM for final report...")
    final_report = generate_final_analysis(combined_filtered_logs)

    print("\nâœ… === FINAL PERFORMANCE ANALYSIS SUMMARY === âœ…\n")
    print(final_report)
