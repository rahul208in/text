
For an offline environment, let's use a simpler embedding approach that doesn't require downloading models. We'll use scikit-learn's TF-IDF vectorizer which works completely offline:

```python
# app/embedding/offline_embedding.py

from sklearn.feature_extraction.text import TfidfVectorizer
from typing import List, Dict
from pathlib import Path
from loguru import logger
from langchain.schema import Document
import pickle
import hashlib

class OfflineEmbeddingHandler:
    """
    Handles document embedding using TF-IDF vectorizer for offline environments
    """
    def __init__(self, cache_dir: str = "cache/embeddings"):
        """
        Initialize the OfflineEmbeddingHandler.
        """
        self.vectorizer = TfidfVectorizer(
            max_features=1024,  # Limit vocabulary size
            stop_words='english'  # Remove common English words
        )
        self.is_fitted = False
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        logger.info("Initialized offline embedding handler")

    def _fit_vectorizer(self, texts: List[str]):
        """Fit the vectorizer on a collection of texts"""
        self.vectorizer.fit(texts)
        self.is_fitted = True

    def get_embedding(self, text: str) -> List[float]:
        """
        Get embedding for a text using TF-IDF
        """
        if not self.is_fitted:
            self._fit_vectorizer([text])
        
        try:
            embedding = self.vectorizer.transform([text]).toarray()[0]
            return embedding.tolist()
        except Exception as e:
            logger.error(f"Error getting embedding: {str(e)}")
            raise

    def embed_documents(self, documents: List[Document]) -> List[Dict]:
        """
        Embed a list of documents.
        """
        # First, fit the vectorizer on all documents
        all_texts = [doc.page_content for doc in documents]
        self._fit_vectorizer(all_texts)

        embedded_documents = []
        for doc in documents:
            try:
                embedding = self.get_embedding(doc.page_content)
                embedded_doc = {
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "embedding": embedding
                }
                embedded_documents.append(embedded_doc)
                logger.info(f"Successfully embedded document: {doc.metadata.get('file_name')}")
            except Exception as e:
                logger.error(f"Error embedding document {doc.metadata.get('file_name')}: {str(e)}")

        return embedded_documents
```

Test script (`test_offline_embedding.py`):

```python
from app.ingestion.ingestion import DataIngestion
from app.embedding.offline_embedding import OfflineEmbeddingHandler

def test_embedding():
    # Initialize DataIngestion
    folder_path = "data/json_files/"
    ingestion = DataIngestion(folder_path)

    # Load documents
    documents = ingestion.load_json_files()

    # Initialize OfflineEmbeddingHandler
    embedding_handler = OfflineEmbeddingHandler()

    # Embed documents
    embedded_docs = embedding_handler.embed_documents(documents)

    # Print results
    for doc in embedded_docs:
        print(f"File: {doc['metadata']['file_name']}")
        print(f"Embedding length: {len(doc['embedding'])}")
        print("-" * 50)

if __name__ == "__main__":
    test_embedding()
```

Key features of this offline implementation:

1. Uses scikit-learn's TF-IDF vectorizer which works completely offline
2. No external model downloads required
3. Generates fixed-length embeddings (1024 dimensions by default)
4. Includes basic text preprocessing (stop word removal)
5. Maintains compatibility with your existing code structure

To use this:

1. Ensure scikit-learn is installed (should be available in your intranet artifactory):
```bash
pip install scikit-learn
```

2. Run the test script:
```bash
python test_offline_embedding.py
```

This solution should work completely offline in your intranet environment. Would you like me to explain any part in more detail?
